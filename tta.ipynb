{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel auto reload modules when the underlying code is chaneged, instead of having to reset the runtime.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as ttf\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "\n",
    "from models.convnext import convnext_t, my_convnext\n",
    "from datasets.classification import ClassificationTestSet\n",
    "from datasets.verification import VerificationDataset\n",
    "from datasets.transform import AlbumTransforms, train_transforms, val_transforms\n",
    "from utils.utils import weight_decay_custom, compute_kl_loss, SAM\n",
    "from run import train, test, inference, face_embedding, verification_inference, gen_cls_submission, gen_ver_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load classification dataset / loader\n",
    "BASE_DIR = '/shared/youngkim/hw2p2'\n",
    "CLS_DIR = os.path.join(BASE_DIR, '11-785-s22-hw2p2-classification')\n",
    "VER_DIR = os.path.join(BASE_DIR, '11-785-s22-hw2p2-verification')\n",
    "\n",
    "CLS_TRAIN_DIR = os.path.join(CLS_DIR, \"classification/classification/train\") # This is a smaller subset of the data. Should change this to classification/classification/train\n",
    "CLS_VAL_DIR = os.path.join(CLS_DIR, \"classification/classification/dev\")\n",
    "CLS_TEST_DIR = os.path.join(CLS_DIR, \"classification/classification/test\")\n",
    "\n",
    "\n",
    "face_norm_mean = (0.511, 0.402, 0.351)\n",
    "face_norm_std = (0.270, 0.235, 0.222)\n",
    "\n",
    "test_transforms = A.Compose(\n",
    "    [\n",
    "        # A.CenterCrop(height=200, width=200),\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(\n",
    "                mean=face_norm_mean, \n",
    "                std=face_norm_std, \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tta_transforms = A.Compose(\n",
    "    [\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, \n",
    "                           scale_limit=0.1, \n",
    "                           rotate_limit=15, \n",
    "                           p=0.5),\n",
    "        A.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=0.5\n",
    "            ),\n",
    "        A.OneOf([\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=10, \n",
    "                sat_shift_limit=15, \n",
    "                val_shift_limit=10, \n",
    "                p=0.5\n",
    "            ),\n",
    "            A.RGBShift(\n",
    "                r_shift_limit=13,\n",
    "                g_shift_limit=10,\n",
    "                b_shift_limit=8,\n",
    "                p=0.5\n",
    "            )\n",
    "        ], p=1),  \n",
    "        A.Normalize(\n",
    "                mean=face_norm_mean, \n",
    "                std=face_norm_std, \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "val_dataset = torchvision.datasets.ImageFolder(CLS_VAL_DIR,\n",
    "                                                transform=AlbumTransforms(tta_transforms))\n",
    "test_dataset = ClassificationTestSet(CLS_TEST_DIR, AlbumTransforms(tta_transforms))\n",
    "\n",
    "valid_loader = DataLoader(val_dataset, batch_size=256, shuffle=False,\n",
    "                        drop_last=False, num_workers=1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False,\n",
    "                        drop_last=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2' if torch.cuda.is_available() else \"cpu\")\n",
    "save_name = 'convnext-4772-16:13:41:23'\n",
    "model = my_convnext(dropout=0, block_nums=[4,7,7,2]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: /shared/youngkim/hw2p2/weights/convnext-4772-16:13:41:23.pth\n"
     ]
    }
   ],
   "source": [
    "weights_path = '/shared/youngkim/hw2p2/weights/'\n",
    "pretrained_path = os.path.join(weights_path, f'{save_name}.pth')\n",
    "checkpoint = torch.load(pretrained_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# start_epoch = checkpoint['epoch'] + 1\n",
    "print(f\"Model loaded: {pretrained_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tta(model, device, num_epochs, test_loader, valid_loader=None):\n",
    "    model.eval()\n",
    "\n",
    "    if valid_loader is not None:\n",
    "        val_cls_out = torch.zeros([35000,7000], device=device, dtype=torch.float32)\n",
    "        true_y_list = []\n",
    "        for epoch in range(num_epochs):\n",
    "            i = 0\n",
    "            for data, true_y in tqdm(valid_loader, total=len(valid_loader), desc='Valid', position=0, leave=True):\n",
    "                data = data.to(device)\n",
    "                true_y = true_y.to(device)                \n",
    "                \n",
    "                with torch.no_grad():   \n",
    "                    output = model(data)\n",
    "                    if i == 0:\n",
    "                        val_out = deepcopy(output)\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        val_out = torch.concat((val_out, output), dim=0)\n",
    "\n",
    "                    if epoch == 0:\n",
    "                        true_y_list.extend(true_y.tolist())\n",
    "        \n",
    "            val_cls_out = val_cls_out + val_out\n",
    "\n",
    "        val_pred_y = torch.argmax(val_cls_out, axis=1)\n",
    "        val_accuracy =  accuracy_score(true_y_list, val_pred_y.tolist())\n",
    "\n",
    "        print(val_accuracy)\n",
    "\n",
    "    test_cls_out = torch.zeros([35000,7000], device=device, dtype=torch.float32)\n",
    "    for epoch in range(num_epochs):\n",
    "        i = 0\n",
    "        for data in tqdm(test_loader, total=len(test_loader), desc='Test', position=0, leave=True):\n",
    "            data = data.to(device)\n",
    "            \n",
    "            with torch.no_grad():   \n",
    "                output = model(data)\n",
    "                if i == 0:\n",
    "                    test_out = deepcopy(output)\n",
    "                    i += 1\n",
    "                else:\n",
    "                    test_out = torch.concat((test_out, output), dim=0)\n",
    "    \n",
    "        test_cls_out = test_cls_out + test_out\n",
    "\n",
    "    pred_y = torch.argmax(test_cls_out, axis=1).tolist()\n",
    "\n",
    "    return pred_y\n",
    "\n",
    "# tta_pred_5epochs = tta(model, device, 5, test_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid: 100%|██████████| 137/137 [00:52<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9078571428571428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_line = tta(model, device, 1, test_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid: 100%|██████████| 137/137 [00:55<00:00,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9066857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "center_crop = tta(model, device, 1, test_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid:   0%|          | 0/137 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "tta_pred_10epochs = tta(model, device, 10, test_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_path = '/shared/youngkim/hw2p2/submissions/'\n",
    "\n",
    "def gen_submission(predictions):\n",
    "    test_names = [str(i).zfill(6) + \".jpg\" for i in range(len(predictions))]\n",
    "    submission = pd.DataFrame(zip(test_names, predictions), columns=['id', 'label'])\n",
    "    submission.to_csv(os.path.join(submissions_path, f'tta-5.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_submission(tta_pred_5epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 541k/541k [00:03<00:00, 160kB/s]\n",
      "Successfully submitted to Face Recognition"
     ]
    }
   ],
   "source": [
    "submission_name = 'tta-5'\n",
    "sub_dir = '/shared/youngkim/hw2p2/submissions'\n",
    "cls_csv = os.path.join(sub_dir, f'{submission_name}.csv')\n",
    "!kaggle competitions submit -c 11-785-s22-hw2p2-classification -f {cls_csv} -m {submission_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3c4dd319ecd10802be1c77dfdfd6b25af6a7d32bbb2d7a1dac30da2e89c7158"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('happywhale')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
